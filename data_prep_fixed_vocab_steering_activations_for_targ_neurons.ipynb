{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "6lBdvwRoxUnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rxxczgCGB9k-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from collections import defaultdict\n",
        "import plotly.graph_objects as go\n",
        "from sae_lens import SAE, HookedSAETransformer\n",
        "from functools import partial\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pickle\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7AQBmsKIOS6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7GFcjwTUB_AP"
      },
      "outputs": [],
      "source": [
        "!pip install sae_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "c8ABnUQZvXUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NZIeVm_bX50f"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMX2ChbBIGrC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR53dQDBDaPW"
      },
      "outputs": [],
      "source": [
        "from sae_lens import (\n",
        "    SAE,\n",
        "    ActivationsStore,\n",
        "    HookedSAETransformer,\n",
        "    LanguageModelSAERunnerConfig,\n",
        "    SAEConfig,\n",
        "    SAETrainingRunner,\n",
        "    upload_saes_to_huggingface,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoeNCVrHva1v"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "ds = concatenate_datasets([ds['train'], ds['validation'], ds['test']])\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlKeT_Dq5vSI"
      },
      "outputs": [],
      "source": [
        "label_names = ds.features[\"labels\"].feature.names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyudmgaT5xtA"
      },
      "outputs": [],
      "source": [
        "targeted_emotions = ['joy', 'anger', 'disgust', 'sadness', 'love', 'fear', 'excitement']\n",
        "labels = []\n",
        "for em in targeted_emotions:\n",
        "  labels.append(label_names.index(em))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3Nrp7LF6bh8"
      },
      "outputs": [],
      "source": [
        "ds = ds.filter(lambda x: any(label in labels for label in x['labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyFJlUuj62p4"
      },
      "outputs": [],
      "source": [
        "ds = ds.filter(lambda x: len(x['labels']) == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRj2pKivh1Nz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "ids = np.load(\"ds_filt.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7DHauwXhyU_"
      },
      "outputs": [],
      "source": [
        "filtered_ds = ds.filter(lambda x: x[\"id\"] in ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmvnxoNfiCyN"
      },
      "outputs": [],
      "source": [
        "ds = filtered_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_qfJsTiqCLA"
      },
      "outputs": [],
      "source": [
        "len(filtered_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVIgUslAtUvl"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xZ7zXL3PxxW"
      },
      "outputs": [],
      "source": [
        "texts = [s['text'] for s in ds]\n",
        "labels = [s['labels'] for s in ds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Z_FRo4bRtG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjTEDv95a0fa"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKLiSEIo_6UJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmk2djsFPqZE"
      },
      "outputs": [],
      "source": [
        "word_counter = Counter()\n",
        "for sentence in texts:\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "    filt_words = [w for w in words if(w not in stop_words) and (len(w)>1)]\n",
        "    word_counter.update(filt_words)\n",
        "\n",
        "word_freq_df = pd.DataFrame(word_counter.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=word_freq_df.head(30), x='word', y='count', palette='viridis')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Top 30 Most Frequent Words in GoEmotions Dataset')\n",
        "plt.xlabel('Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fJgVdI9_y4r"
      },
      "outputs": [],
      "source": [
        "word_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDruIqUl_laC"
      },
      "outputs": [],
      "source": [
        "new = [w for w, c in word_counter.items() if c > 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1G1zN8__kZq"
      },
      "outputs": [],
      "source": [
        "len(new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ZUfjKKYL8-"
      },
      "source": [
        "## Classification with Gemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLqVc3DWYQcS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fus4ZeCmy65d"
      },
      "outputs": [],
      "source": [
        "def build_prompt(shots=('joy', 'sadness'), prompt_index=1):\n",
        "    sample_pool = {\n",
        "        'joy': 'My first child was born.',\n",
        "        'anger': 'My husband missed an important call, because his phone was on silent AGAIN!',\n",
        "        'disgust': 'I saw mouldy food.',\n",
        "        'sadness': 'My dog died last week.',\n",
        "        'love': 'I told my partner I loved them.',\n",
        "        'fear': 'I was confronted by a thief.',\n",
        "        'excitement': 'I got an A in my exam!'\n",
        "    }\n",
        "\n",
        "    # Filter and prepare few-shot examples from the selected emotions\n",
        "    selected_shots = [(sample_pool[e], e) for e in shots if e in sample_pool]\n",
        "\n",
        "    emotion_list = \"Consider this list of emotions: joy, anger, disgust, sadness, love, fear, excitement. \"\n",
        "\n",
        "    templates = [\n",
        "        \"What are the inferred emotions in the following contexts?\",\n",
        "        emotion_list + \"What are the inferred emotions in the following contexts?\",\n",
        "        \"\",\n",
        "        \"Guess the emotion.\",\n",
        "        \"Decipher the emotion from the following statements: \",\n",
        "        \"Decipher the label for the following statements: \",\n",
        "        \"What is the label, for the statement? \",\n",
        "        \"What is the label, given the context? \",\n",
        "        emotion_list + \"Decipher the emotion from the following statements: \",\n",
        "        emotion_list + \"Decipher the label for the following statements: \",\n",
        "    ]\n",
        "\n",
        "    header = templates[prompt_index]\n",
        "    body = header.strip()\n",
        "\n",
        "    for text, emotion in selected_shots:\n",
        "        body += f\" Context: {text} Answer: {emotion}\"\n",
        "\n",
        "    return lambda x: f\"{body} Context: {x} Answer:\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBzueSovYbwS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def classify_with_gemma(ds, tokenizer, model, targeted_emotions, prompt_index=1, shot_emotions=None):\n",
        "    if shot_emotions is None:\n",
        "        shot_emotions = targeted_emotions[:7]\n",
        "\n",
        "    prompt_func = build_prompt(shots=shot_emotions, prompt_index=prompt_index)\n",
        "    preds = []\n",
        "\n",
        "    for example in tqdm(ds, desc=\"Classifying with Gemma (few-shot)\"):\n",
        "        text = example['text']\n",
        "        prompt = prompt_func(text)\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=3, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        #print(decoded)\n",
        "        pred = decoded[len(prompt):].strip().split()[0].lower()\n",
        "\n",
        "        if pred in targeted_emotions:\n",
        "            preds.append(targeted_emotions.index(pred))\n",
        "        else:\n",
        "            preds.append(\"other\")\n",
        "\n",
        "\n",
        "    return preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxwMckamael0"
      },
      "outputs": [],
      "source": [
        "preds_model = classify_with_gemma(ds, tokenizer, model, targeted_emotions, prompt_index=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaW0T9KbamCL"
      },
      "outputs": [],
      "source": [
        "preds_filtered = []\n",
        "for pred in preds_model:\n",
        "    if isinstance(pred, int):\n",
        "        emotion = targeted_emotions[pred]\n",
        "        if emotion in label_names:\n",
        "            preds_filtered.append(label_names.index(emotion))  # map to ds label ID\n",
        "        else:\n",
        "            preds_filtered.append(\"-1\")  # or -1, or skip it\n",
        "    else:\n",
        "        preds_filtered.append(\"-1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTOHu5SteuYw"
      },
      "outputs": [],
      "source": [
        "texts = [item['text'] for item in ds ]\n",
        "labels = [item['labels'] for item in ds ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyRWJhTSarQW"
      },
      "outputs": [],
      "source": [
        "labels_flat = [lbl[0] for lbl in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_RgQ_U2aq4x"
      },
      "outputs": [],
      "source": [
        "labels_flat = [int(x) for x in labels_flat]\n",
        "preds_filtered = [int(x) for x in preds_filtered]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICWBlQ6b7ZjZ"
      },
      "outputs": [],
      "source": [
        "ds_1 = ds.add_column(\"pred\", preds_filtered)\n",
        "ds_1 = ds_1.add_column('true', labels_flat)\n",
        "ds_filtered = ds_1.filter(lambda x: x[\"true\"] == x[\"pred\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSx_GEaufj7_"
      },
      "outputs": [],
      "source": [
        "ds_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1gmsXw6e1SY"
      },
      "outputs": [],
      "source": [
        "ids = [item['id'] for item in ds_filtered]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck8kErlze9i2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "np.save(\"ds_filt.npy\", ids)\n",
        "files.download(\"ds_filt.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbJvOLvqfc-l"
      },
      "source": [
        "### Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRKge2EO9i-t"
      },
      "outputs": [],
      "source": [
        "true = [label_names[l] for l in labels_flat]\n",
        "pred = [label_names[l] for l in preds_filtered]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddnF1olO9L2m"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(true, pred, labels=targeted_emotions)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_mat, annot=True,\n",
        "            xticklabels=targeted_emotions,\n",
        "            yticklabels=targeted_emotions,\n",
        "            fmt='d', cmap='Blues')\n",
        "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.2%})\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDOnovIExBYA"
      },
      "outputs": [],
      "source": [
        "def plot_sankey(true_indices, pred_indices, targ_em, label_names):\n",
        "\n",
        "    true_labels = [label_names[i] for i in true_indices]\n",
        "    pred_labels = [label_names[i] for i in pred_indices]\n",
        "\n",
        "    all_emotions = sorted(set(true_labels + pred_labels))\n",
        "\n",
        "    label_list = all_emotions + [f\"pred_{l}\" for l in all_emotions]\n",
        "    label_idx = {l: i for i, l in enumerate(label_list)}\n",
        "\n",
        "    emotion_to_color = {\n",
        "        'joy': \"#FF0000\",\n",
        "        'anger': \"#FFA500\",\n",
        "        'sadness': \"#FFFF00\",\n",
        "        'fear': \"#27ae60\",\n",
        "        'disgust': \"#7f8c8d\",\n",
        "        'love': \"#e91e63\",\n",
        "        'excitement': \"#f39c12\",\n",
        "        'neutral': \"#bdc3c7\"\n",
        "    }\n",
        "\n",
        "    node_colors = [emotion_to_color.get(e, \"#CCCCCC\") for e in all_emotions]\n",
        "    node_colors += node_colors\n",
        "\n",
        "\n",
        "    counter = defaultdict(int)\n",
        "    for t, p in zip(true_labels, pred_labels):\n",
        "        src = label_idx[t]\n",
        "        tgt = label_idx[f\"pred_{p}\"]\n",
        "        counter[(src, tgt)] += 1\n",
        "\n",
        "    link_source, link_target, link_value, link_color = [], [], [], []\n",
        "\n",
        "    all_values = list(counter.values())\n",
        "    max_val = max(all_values)\n",
        "\n",
        "    for (src, tgt), val in counter.items():\n",
        "        link_source.append(src)\n",
        "        link_target.append(tgt)\n",
        "        link_value.append(val)\n",
        "\n",
        "        norm_alpha = min(1.0, max(0.2, val / max_val))\n",
        "\n",
        "        hex_color = node_colors[src].lstrip('#')\n",
        "        r, g, b = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "        link_color.append(f'rgba({r},{g},{b},{norm_alpha:.2f})')\n",
        "\n",
        "\n",
        "    fig = go.Figure(go.Sankey(\n",
        "        arrangement=\"snap\",\n",
        "        node=dict(\n",
        "            pad=15,\n",
        "            thickness=25,\n",
        "            line=dict(color=\"black\", width=0.5),\n",
        "            label=label_list,\n",
        "            color=node_colors\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=link_source,\n",
        "            target=link_target,\n",
        "            value=link_value,\n",
        "            color=link_color,\n",
        "        )\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title_text=\"Emotion Classification Flow (True → Predicted)\",\n",
        "        font_size=13,\n",
        "        margin=dict(l=40, r=40, t=50, b=40),\n",
        "        width=600,\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sankey(labels_flat, preds_filtered, targeted_emotions, label_names)"
      ],
      "metadata": {
        "id": "67oTjTfQtRg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb27R3yAYG4K"
      },
      "source": [
        "# Collecting activations for targeted neurons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXBvm7N75cM9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNHevTl45rRo"
      },
      "outputs": [],
      "source": [
        "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "gemma_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=\"layer_20/width_16k/average_l0_71\",\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "gemma = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LiCTwHeCl5e"
      },
      "outputs": [],
      "source": [
        "texts = [item['text'] for item in ds ]\n",
        "labels = [item['labels'] for item in ds ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6toEpIMWRJWi"
      },
      "outputs": [],
      "source": [
        "with open(\"emotional_n_02_thresh.pkl\", \"rb\") as f:\n",
        "    neurons = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n33xSJGJRJY5"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "max_length = 400\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
        "    try:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Get batch\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"].to(device)\n",
        "\n",
        "        # Forward pass with SAE and cache activations\n",
        "        _, cache = gemma.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[gemma_sae],\n",
        "            stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"],\n",
        "        )\n",
        "\n",
        "        # SAE activations (features)\n",
        "        sae_acts = cache[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"]  # [B, T, F]\n",
        "        final_acts = sae_acts[:, -1, :].detach().cpu()\n",
        "\n",
        "        # Sparsity = number of active features\n",
        "        sparsity = (sae_acts[:, -1, :] > 1).sum(dim=-1)  # [B]\n",
        "\n",
        "        decoded_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids]\n",
        "\n",
        "\n",
        "        for i in range(len(batch_texts)):\n",
        "            results.append({\n",
        "                \"input_ids\": input_ids[i].detach().cpu(),              # torch.Tensor\n",
        "                \"tokens\": decoded_tokens[i],                          # list of strings\n",
        "                \"activation\": final_acts[i],                          # torch.Tensor\n",
        "                \"sparsity\": int(sparsity[i]),                         # int\n",
        "            })\n",
        "\n",
        "        # Cleanup\n",
        "        del cache, sae_acts, final_acts, sparsity, input_ids, tokenized\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"⚠️ OOM on batch {i}-{i+batch_size}: {e}\")\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CkhIlR2VHsz"
      },
      "outputs": [],
      "source": [
        "for r, l in zip(results, labels):\n",
        "  r['label'] = l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWshVrXdV7B3"
      },
      "outputs": [],
      "source": [
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIujP2wQVYIX"
      },
      "outputs": [],
      "source": [
        "with open(\"sae_results.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JKSBZdh_Nfe"
      },
      "outputs": [],
      "source": [
        "targ_neurons = [13324, 14857, 2438, 12881, 4560, 1898, 8366, 7077, 8094, 3232, 6953, 6953, 13324, 4456, 7077, 808, 230, 281, 8783, 4305, 7717, 230, 7688, 15261, 4305, 3636, 4326, 11491, 4305, 5413, 9618, 15539]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL4drthW_DSH"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "max_length = 40\n",
        "activation_threshold = 1.0\n",
        "targ_neurons = set(targ_neurons)\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"].to(device)\n",
        "\n",
        "        _, cache = gemma.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[gemma_sae],\n",
        "            stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"],\n",
        "        )\n",
        "\n",
        "        sae_acts = cache[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"].detach().cpu()  # [B, T, F]\n",
        "        final_acts = sae_acts[:, -1, :]  # [B, F]\n",
        "        decoded_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids]\n",
        "\n",
        "        for j in range(len(batch_texts)):\n",
        "            token_acts = sae_acts[j]  # [T, F]\n",
        "            filtered_entries = []\n",
        "\n",
        "            for t in range(token_acts.shape[0]):  # iterate over token positions\n",
        "                for f in targ_neurons:\n",
        "                    act_val = token_acts[t, f].item()\n",
        "                    if act_val > activation_threshold:\n",
        "                        filtered_entries.append((t, f, act_val))\n",
        "\n",
        "            final_filtered = {\n",
        "                f: final_acts[j, f].item()\n",
        "                for f in targ_neurons\n",
        "                if final_acts[j, f].item() > activation_threshold\n",
        "            }\n",
        "\n",
        "            results.append({\n",
        "                \"input_ids\": input_ids[j].detach().cpu(),\n",
        "                \"tokens\": decoded_tokens[j],\n",
        "                \"activation_targeted\": final_filtered,\n",
        "                \"active_neurons\": filtered_entries,\n",
        "                \"sparsity\": len(filtered_entries),\n",
        "            })\n",
        "\n",
        "        del cache, sae_acts, final_acts, input_ids, tokenized\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF9Q7I6oElR1"
      },
      "outputs": [],
      "source": [
        "for r, l in zip(results, labels):\n",
        "  r['label'] = l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g9b4BfXEv7a"
      },
      "outputs": [],
      "source": [
        "with open(\"sae_results_tokens_top_k.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwi8F4Jsc7Qv"
      },
      "source": [
        "# fixed vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ETnSNbNdEDw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KicrMGumdEDx"
      },
      "outputs": [],
      "source": [
        "from sae_lens import SAE, ActivationsStore, HookedSAETransformer\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "gemma_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=\"layer_20/width_16k/average_l0_71\",\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "gemma = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQzliyqmeegK"
      },
      "outputs": [],
      "source": [
        "vocab_set = set()\n",
        "\n",
        "with open(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if parts:\n",
        "            vocab_set.add(parts[0])\n",
        "\n",
        "print(\"Total unique words:\", len(vocab_set))\n",
        "print(\"Sample:\", list(vocab_set)[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LBO5u5G1VvX"
      },
      "outputs": [],
      "source": [
        "# version where at least 1 emotion labeled\n",
        "\n",
        "filtered_vocab = set()\n",
        "\n",
        "with open(\"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        word, emotion, label = line.strip().split(\"\\t\")\n",
        "        if label == \"1\":\n",
        "            filtered_vocab.add(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTBl5tWqd2B-"
      },
      "outputs": [],
      "source": [
        "len(filtered_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqTTNiV61qCi"
      },
      "outputs": [],
      "source": [
        "vocab_set = filtered_vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83IMUPAydJ4d"
      },
      "outputs": [],
      "source": [
        "for word, count in word_counter.items():\n",
        "  filtered_vocab.add(word)\n",
        "\n",
        "\n",
        "filtered_vocab = list(filtered_vocab)\n",
        "vocab_set = filtered_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qz_8oeMjx28"
      },
      "outputs": [],
      "source": [
        "len(vocab_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvl23Fgudaiu"
      },
      "outputs": [],
      "source": [
        "with open(\"emotional_n_02_thresh.pkl\", \"rb\") as f:\n",
        "    neurons = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsM2Nlwre6pL"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "max_length = 40\n",
        "activation_threshold = 0.0\n",
        "word_to_activations = defaultdict(list)\n",
        "\n",
        "texts = [s[\"text\"] for s in ds]\n",
        "\n",
        "word_pattern = re.compile(r'\\b({})\\b'.format('|'.join(map(re.escape, vocab_set))), flags=re.IGNORECASE)\n",
        "word_index_map = defaultdict(list)\n",
        "\n",
        "# Build inverted index: word -> list of sentence indices where it appears\n",
        "for idx, sentence in tqdm(enumerate(texts), total=len(texts), desc=\"Indexing sentences\"):\n",
        "    matches = word_pattern.findall(sentence)\n",
        "    for word in set(matches):\n",
        "        word_index_map[word.lower()].append(idx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlNUpVcetIv3"
      },
      "outputs": [],
      "source": [
        "def collect_word_activations(texts, vocab_set, tokenizer, gemma, gemma_sae, device, target_neurons,\n",
        "                             batch_size=4, max_length=40, activation_threshold=1.0, top_k=5):\n",
        "\n",
        "    word_to_activations = defaultdict(list)\n",
        "    word_to_high_acts = defaultdict(list)\n",
        "    word_pattern = re.compile(r'\\b({})\\b'.format('|'.join(map(re.escape, vocab_set))), flags=re.IGNORECASE)\n",
        "\n",
        "    # Map vocab words to dataset indices\n",
        "    word_index_map = defaultdict(list)\n",
        "    for idx, sentence in tqdm(enumerate(texts), total=len(texts), desc=\"Indexing sentences\"):\n",
        "        matches = word_pattern.findall(sentence)\n",
        "        for word in set(matches):\n",
        "            word_index_map[word.lower()].append(idx)\n",
        "\n",
        "    for word in tqdm(vocab_set, desc=\"Processing vocab words\"):\n",
        "        word = word.lower()\n",
        "        indices = word_index_map.get(word, [])[:20]\n",
        "        if not indices:\n",
        "            continue\n",
        "\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            batch_texts = [texts[j] for j in batch_indices]\n",
        "\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            tokenized = tokenizer(\n",
        "                batch_texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length\n",
        "            )\n",
        "            input_ids = tokenized[\"input_ids\"].to(device)\n",
        "\n",
        "            try:\n",
        "                _, cache = gemma.run_with_cache_with_saes(\n",
        "                    input_ids,\n",
        "                    saes=[gemma_sae],\n",
        "                    stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "                    names_filter=[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"],\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping batch due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "            sae_acts = cache[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"].detach().cpu()  # [B, T, F]\n",
        "\n",
        "            for b in range(sae_acts.shape[0]):\n",
        "                tokens = tokenizer.convert_ids_to_tokens(input_ids[b])\n",
        "                token_acts = sae_acts[b]  # [T, F]\n",
        "                word_pos = [t for t, tok in enumerate(tokens) if word in tok.lower()]\n",
        "                if not word_pos:\n",
        "                    continue\n",
        "\n",
        "                # Mean over all positions where word appears → vector representation\n",
        "                mean_vector = token_acts[word_pos].mean(dim=0)[target_neurons].numpy()\n",
        "                word_to_activations[word].append(mean_vector)\n",
        "\n",
        "                # Collect all high-activation events for interpretability\n",
        "                for t in word_pos:\n",
        "                    for f in target_neurons:\n",
        "                        val = token_acts[t, f].item()\n",
        "                        if val > activation_threshold:\n",
        "                            word_to_high_acts[word].append((tokens[t], f, val))\n",
        "\n",
        "    # Final top-k mean vector per word (filtering out zero vectors)\n",
        "    def topk_mean(arrs, k=top_k):\n",
        "        if len(arrs) <= k:\n",
        "            return np.mean(arrs, axis=0)\n",
        "        scores = [np.linalg.norm(vec) for vec in arrs]\n",
        "        topk_indices = np.argsort(scores)[-k:]\n",
        "        topk_vecs = [arrs[i] for i in topk_indices]\n",
        "        return np.mean(topk_vecs, axis=0)\n",
        "\n",
        "    word_to_sae_vec = {\n",
        "        w: topk_mean(vectors)\n",
        "        for w, vectors in word_to_activations.items()\n",
        "        if vectors\n",
        "    }\n",
        "\n",
        "    return word_to_sae_vec, word_to_high_acts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMRFTcjrtO4u"
      },
      "outputs": [],
      "source": [
        "word_to_sae_vec, word_to_high_acts = collect_word_activations(\n",
        "    texts=texts,\n",
        "    vocab_set=vocab_set,\n",
        "    tokenizer=tokenizer,\n",
        "    gemma=gemma,\n",
        "    gemma_sae=gemma_sae,\n",
        "    device=device,\n",
        "    target_neurons=list(neurons),\n",
        "    batch_size=4,\n",
        "    max_length=40,\n",
        "    activation_threshold=1.0,\n",
        "    top_k=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7ZHrPtBvRTz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def extract_top_words_per_neuron(word_to_sae_vec, target_neurons, top_k=10, diversity=True, min_score_threshold=5.0):\n",
        "    top_words_per_neuron = {}\n",
        "    neuron_to_index = {n: i for i, n in enumerate(target_neurons)}\n",
        "    all_words = list(word_to_sae_vec.keys())\n",
        "    vectors = np.stack([word_to_sae_vec[w] for w in all_words])  # shape: [#words, #neurons]\n",
        "\n",
        "    for neuron in target_neurons:\n",
        "        idx = neuron_to_index[neuron]\n",
        "        scores = vectors[:, idx]\n",
        "        sorted_indices = np.argsort(-scores)\n",
        "\n",
        "        top_words = []\n",
        "        used_vecs = []\n",
        "\n",
        "        for i in sorted_indices:\n",
        "            score = scores[i]\n",
        "            if score < min_score_threshold:\n",
        "                break  # stop early if score drops below threshold\n",
        "\n",
        "            word = all_words[i]\n",
        "            vec = word_to_sae_vec[word]\n",
        "\n",
        "            if not diversity or not used_vecs:\n",
        "                top_words.append((word, score))\n",
        "                used_vecs.append(vec)\n",
        "            else:\n",
        "                sims = cosine_similarity([vec], used_vecs)[0]\n",
        "                if np.max(sims) < 0.9:\n",
        "                    top_words.append((word, score))\n",
        "                    used_vecs.append(vec)\n",
        "\n",
        "            if len(top_words) >= top_k:\n",
        "                break\n",
        "\n",
        "        if top_words:\n",
        "            print(f\"Neuron {neuron} → Collected {len(top_words)} words (min activation = {top_words[-1][1]:.4f})\")\n",
        "            top_words_per_neuron[neuron] = top_words\n",
        "        else:\n",
        "            print(f\"Neuron {neuron} → No words above threshold ({min_score_threshold})\")\n",
        "\n",
        "    return top_words_per_neuron\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH8m-5gtvSWX"
      },
      "outputs": [],
      "source": [
        "top_words_per_neuron = extract_top_words_per_neuron(word_to_sae_vec, neurons, top_k=30, diversity=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgRhrocdymgn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open(\"word_to_high_acts_drop_stop_30.json\", \"w\") as f:\n",
        "    json.dump(word_to_high_acts, f)\n",
        "\n",
        "with open(\"word_to_sae_vec_reduced_drop_stop_30.json\", \"w\") as f:\n",
        "    json.dump({k: v.tolist() for k, v in word_to_sae_vec.items()}, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQKGwYvwRzcD"
      },
      "outputs": [],
      "source": [
        "word_to_sae_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bShQFBVzwr2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Convert activation values to float and structure for JSON\n",
        "top_words_serializable = {\n",
        "    int(neuron): [{\"word\": w, \"activation\": float(a)} for w, a in word_list]\n",
        "    for neuron, word_list in top_words_per_neuron.items()\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open(\"top_words_per_neuron_drop_stop_30.json\", \"w\") as f:\n",
        "    json.dump(top_words_serializable, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Us7Y5Q0z28y"
      },
      "outputs": [],
      "source": [
        "top_words_per_neuron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-UB2a-9xLXt"
      },
      "source": [
        "# Steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZa5XsTixLXt"
      },
      "outputs": [],
      "source": [
        "# 1. Set device and load models\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "gemma_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=\"layer_20/width_16k/average_l0_71\",\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "gemma = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPzO4RiDxLXt"
      },
      "outputs": [],
      "source": [
        "max_act_df = pd.read_csv('/content/max_activations_for_targ_neurons.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_l147n5xLXt"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYY7nD7cxLXt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from sae_lens import ActivationsStore\n",
        "\n",
        "# Create the activation store from your dataset\n",
        "activation_store = ActivationsStore.from_sae(\n",
        "    model=gemma,\n",
        "    sae=gemma_sae,\n",
        "    streaming=True,\n",
        "    store_batch_size_prompts=8,\n",
        "    train_batch_size_tokens=4096,\n",
        "    n_batches_in_buffer=4,\n",
        "    device=gemma.cfg.device,\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvhwz19RxLXt"
      },
      "outputs": [],
      "source": [
        "# === Define your anger neuron index ===\n",
        "anger_neuron_idx = 2438  # replace with actual anger-selective neuron index\n",
        "\n",
        "# === Function to find max activation of this neuron ===\n",
        "def find_max_activation(model, sae, act_store, neuron_idx, num_batches=100):\n",
        "    max_activation = 0.0\n",
        "    pbar = tqdm(range(num_batches), desc=\"Finding max activation\")\n",
        "\n",
        "    for _ in pbar:\n",
        "        tokens = act_store.get_batch_tokens()\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            tokens,\n",
        "            saes=[sae],\n",
        "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        )\n",
        "        acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        acts_flat = acts.flatten(0, 1)\n",
        "        batch_max = acts_flat[:, neuron_idx].max().item()\n",
        "        max_activation = max(max_activation, batch_max)\n",
        "        pbar.set_description(f\"Max activation: {max_activation:.2f}\")\n",
        "\n",
        "    return max_activation\n",
        "\n",
        "# === Hook for steering ===\n",
        "def steering_hook_fn(resid_pre, hook, steering_vector, strength, max_act):\n",
        "    return resid_pre + max_act * strength * steering_vector\n",
        "\n",
        "# === Generate with steering ===\n",
        "def generate_with_steering(model, sae, prompt, neuron_idx, max_act, strength=1.0, max_new_tokens=10):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    steer_vec = sae.W_dec[neuron_idx].to(model.cfg.device)\n",
        "\n",
        "    hook_fn = partial(\n",
        "        steering_hook_fn,\n",
        "        steering_vector=steer_vec,\n",
        "        strength=strength,\n",
        "        max_act=max_act\n",
        "    )\n",
        "\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, hook_fn)]):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output_ids[0])\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "# === Hook for ablation ===\n",
        "def ablation_hook_fn(resid_pre, hook, ablate_vector):\n",
        "    return resid_pre - ablate_vector  # Subtract the contribution of this neuron\n",
        "\n",
        "# === Generate with ablation ===\n",
        "def generate_with_ablation(model, sae, prompt, neuron_idx, max_act, max_new_tokens=10):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    # Compute the vector to subtract: max activation × SAE decoder direction\n",
        "    ablate_vec = max_act * sae.W_dec[neuron_idx].to(model.cfg.device)\n",
        "\n",
        "    hook_fn = partial(\n",
        "        ablation_hook_fn,\n",
        "        ablate_vector=ablate_vec\n",
        "    )\n",
        "\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, hook_fn)]):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "# === Hook that zeroes out specific neuron(s) in SAE latent space ===\n",
        "def latent_ablation_hook_fn(sae_acts, hook, neuron_idx):\n",
        "    sae_acts[:, -1, neuron_idx] = 0  # zero only the last token activation\n",
        "    return sae_acts\n",
        "\n",
        "# === Run generation with latent ablation ===\n",
        "def generate_with_sae_ablation(model, sae, prompt, neuron_idx, max_new_tokens=20):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    # Step 1: Run with SAE cache\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[sae],\n",
        "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        )\n",
        "\n",
        "    # Step 2: Get and modify SAE activations (zero the target neuron)\n",
        "    sae_acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "    sae_acts[:, -1, neuron_idx] = 0\n",
        "\n",
        "    # Step 3: Reconstruct patch for residual stream\n",
        "    sae_patch = sae_acts[:, -1, :] @ sae.W_dec\n",
        "\n",
        "    # Step 4: Hook to patch the residual stream\n",
        "    def patch_resid(resid, hook):\n",
        "        resid[:, -1, :] += sae_patch\n",
        "        return resid\n",
        "\n",
        "    # Step 5: Generate with patched residual stream\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, patch_resid)]):\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW1vTtmsxLXu"
      },
      "outputs": [],
      "source": [
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22-KmCWbxLXu"
      },
      "outputs": [],
      "source": [
        "# === Hook for steering ===\n",
        "def steering_hook_fn(resid_pre, hook, steering_vector, strength, max_act):\n",
        "    return resid_pre + max_act * strength * steering_vector\n",
        "\n",
        "# === Generate with steering ===\n",
        "def generate_with_steering(model, sae, prompt, neuron_indices, max_act, strength=1.0, max_new_tokens=10):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    if isinstance(neuron_indices, int):\n",
        "        neuron_indices = [neuron_indices]\n",
        "\n",
        "    # Combine decoded vectors of all neurons\n",
        "    steer_vecs = sae.W_dec[neuron_indices].to(model.cfg.device)  # [N, d_model]\n",
        "    steering_vector = steer_vecs.sum(dim=0)  # Alternatively, use .mean(dim=0)\n",
        "\n",
        "    # Build hook\n",
        "    hook_fn = partial(\n",
        "        steering_hook_fn,\n",
        "        steering_vector=steering_vector,\n",
        "        strength=strength,\n",
        "        max_act=max_act\n",
        "    )\n",
        "\n",
        "    # Apply hook and generate\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, hook_fn)]):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXef9dM3xLXu"
      },
      "outputs": [],
      "source": [
        "def clean_and_shorten(text):\n",
        "    # Remove special tokens like <bos> and the prompt\n",
        "    cleaned = text.replace(\"<bos>\", \"\").strip()\n",
        "    prompt_prefix = \"I can't believe that you said it to me:\"\n",
        "    if cleaned.lower().startswith(prompt_prefix.lower()):\n",
        "        cleaned = cleaned[len(prompt_prefix):].strip()\n",
        "\n",
        "    match = re.search(r\"^(.*?[.!?])(?=\\s|\\n|$)\", cleaned + (\" \" or \"\\n\"))\n",
        "    return match.group(1).strip() if match else cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_MkhPqjxLXu"
      },
      "outputs": [],
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLxw-FjqxLXu"
      },
      "outputs": [],
      "source": [
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "vader_lex = sia.lexicon\n",
        "\n",
        "# 1. Extended slang/swear mapping: variant → base word in VADER\n",
        "variant_to_base = {\n",
        "    # Strong Negative\n",
        "    'fucking': 'fuck',\n",
        "    'fucked': 'fuck',\n",
        "    'motherfucker': 'fuck',\n",
        "    'asshole': 'shit',\n",
        "    'douche': 'shit',\n",
        "    'douchebag': 'shit',\n",
        "    'bullshit': 'shit',\n",
        "    'jerk': 'jerk',\n",
        "    'bitches': 'bitch',\n",
        "    'cunt': 'bitch',\n",
        "    'slut': 'bitch',\n",
        "    'whore': 'bitch',\n",
        "    'twat': 'bitch',\n",
        "    'pussy': 'bitch',\n",
        "    'moron': 'idiot',\n",
        "    'retard': 'idiot',\n",
        "    'stupid': 'idiot',\n",
        "    'dumbass': 'idiot',\n",
        "    'loser': 'idiot',\n",
        "    'trash': 'idiot',\n",
        "    'cringe': 'lame',\n",
        "    'pathetic': 'lame',\n",
        "    'toxic': 'bad',\n",
        "    'ew': 'bad',\n",
        "    'meh': 'bad',\n",
        "    'wtf': 'damn',\n",
        "    'creepy': 'scary',     # fixed\n",
        "    'ugly': 'bad',\n",
        "    'nasty': 'bad',\n",
        "    'deadinside' : 'depressing',\n",
        "\n",
        "    # Positive Slang (re-mapped to valid VADER bases)\n",
        "    'queen': 'amazing',\n",
        "    'king': 'amazing',\n",
        "    'slay': 'amazing',\n",
        "    'boss': 'amazing',\n",
        "    'icon': 'amazing',\n",
        "    'legend': 'amazing',\n",
        "    'goddess': 'amazing',\n",
        "    'goat': 'great',\n",
        "    'goated': 'great',\n",
        "    'banger': 'awesome',\n",
        "    'fire': 'awesome',\n",
        "    'based': 'awesome',\n",
        "    'lit': 'awesome',\n",
        "    'dope': 'awesome',\n",
        "    'hella': 'good',\n",
        "    'savage': 'strong',\n",
        "    'cute': 'sweet',\n",
        "    'adorable': 'sweet',\n",
        "    'fine': 'nice',\n",
        "    'hot': 'nice',\n",
        "    'sexy': 'nice',\n",
        "    'clean': 'nice',\n",
        "    'smooth': 'nice',\n",
        "    'beautiful': 'nice',\n",
        "    'pretty': 'sweet',\n",
        "\n",
        "    # Love/excitement slang\n",
        "    'loveee': 'love',\n",
        "    'lovin': 'love',\n",
        "    'obsessed': 'love',\n",
        "    'crushing': 'love',\n",
        "    'crushin': 'love',\n",
        "    'inlove': 'love',\n",
        "    'cutie': 'sweet',\n",
        "    'sweetie': 'sweet',\n",
        "    'bby': 'sweet',\n",
        "    'boo': 'sweet',\n",
        "    'bae': 'sweet',\n",
        "    'ily': 'love',\n",
        "    'ily2': 'love',\n",
        "    'xoxo': 'love',\n",
        "\n",
        "    # Casual/slang humor or approval\n",
        "    'deadass': 'serious',\n",
        "    'fr': 'serious',\n",
        "    'bruh': 'funny',\n",
        "    'lmao': 'funny',\n",
        "    'rofl': 'funny',\n",
        "    'lol': 'funny',\n",
        "    'omg': 'wow',\n",
        "    'vibing': 'happy',\n",
        "    'vibe': 'happy',\n",
        "    'energy': 'happy',\n",
        "\n",
        "    # Sadness / Depression (slangified)\n",
        "    'sadge': 'sad',\n",
        "    'deadinside': 'depressing',\n",
        "    'cryin': 'sad',\n",
        "    'cryinggg': 'sad',\n",
        "    'sobbing': 'sad',\n",
        "    'nooo': 'sad',\n",
        "    'ughhh': 'sad',\n",
        "    'mentallyill': 'depressing',\n",
        "    'depr3ssed': 'depressing',\n",
        "    'downbad': 'sad',\n",
        "    'voidcore': 'depressing',\n",
        "    'brainrotted': 'depressing',\n",
        "    'overit': 'sad',\n",
        "    'can’ttakeit': 'depressing',\n",
        "    'emptyaf': 'sad',\n",
        "    'selfhatin': 'bad',\n",
        "\n",
        "    # Anxiety / Fear / Panic (slangified)\n",
        "    'scaredaf': 'scary',\n",
        "    'panikin': 'scary',\n",
        "    'anxiousss': 'scary',\n",
        "    'stressing': 'scary',\n",
        "    'freakinout': 'scary',\n",
        "    'paranoidd': 'scary',\n",
        "    'helplessss': 'sad',\n",
        "    'losingit': 'scary',\n",
        "    'nervousaf': 'scary',\n",
        "    'shaking': 'scary',\n",
        "    'brainmelting': 'scary',\n",
        "\n",
        "    # Disgust / Repulsion (slangified)\n",
        "    'eww': 'gross',\n",
        "    'vom': 'gross',\n",
        "    'nastyyy': 'gross',\n",
        "    'disgustinn': 'gross',\n",
        "    'cringeaf': 'gross',\n",
        "    'icky': 'gross',\n",
        "    'yuck': 'gross',\n",
        "    'throwingup': 'gross',\n",
        "    'grossedout': 'gross',\n",
        "    'gagging': 'gross',\n",
        "\n",
        "    # Joy / Affection / Love / Excitement (slangified)\n",
        "    'adorbs': 'sweet',\n",
        "    'cutiepie': 'sweet',\n",
        "    'angelbaby': 'sweet',\n",
        "    'sunshiny': 'happy',\n",
        "    'preciousaf': 'sweet',\n",
        "    'ilysm': 'love',\n",
        "    'ily2': 'love',\n",
        "    'lovinggg': 'love',\n",
        "    'obsessssed': 'love',\n",
        "    'snuggly': 'love',\n",
        "    'heartmelt': 'love',\n",
        "    'blessedaf': 'grateful',\n",
        "    'hypeddd': 'excited',\n",
        "    'vibinggg': 'happy',\n",
        "    'ecstaticcc': 'happy',\n",
        "    'excitedd': 'excited',\n",
        "    'inloveee': 'love',\n",
        "    'crushinnn': 'love',\n",
        "    'cutenessoverload': 'sweet',\n",
        "    'hearteyes': 'love',\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 2. Build adjusted lexicon using VADER scores\n",
        "adjusted_custom_lexicon = {}\n",
        "\n",
        "for word, base in variant_to_base.items():\n",
        "    base_score = vader_lex.get(base)\n",
        "    if base_score is not None:\n",
        "        if word not in vader_lex:\n",
        "            adjusted_custom_lexicon[word] = base_score\n",
        "        else:\n",
        "            print(f\"⏩ '{word}' already in VADER — skipping override.\")\n",
        "    else:\n",
        "        print(f\"⚠️ Base word '{base}' not found in VADER for '{word}' — skipping.\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Update VADER with these custom words\n",
        "sia.lexicon.update(adjusted_custom_lexicon)\n",
        "\n",
        "# 4. Test the result\n",
        "examples = [\n",
        "    \"You are a fucking asshole.\",\n",
        "    \"That song is an absolute banger!\",\n",
        "    \"Stop being so cringe.\",\n",
        "    \"She's a queen. Totally goated.\",\n",
        "    \"This is such bullshit.\",\n",
        "    \"I'm deadass serious.\",\n",
        "    \"That guy is a total douchebag.\",\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    print(f\"→ {ex}\")\n",
        "    print(sia.polarity_scores(ex))\n",
        "    print('-' * 40)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PzATG9BxLXu"
      },
      "outputs": [],
      "source": [
        "disgust_neurons = [4456, 6953, 13324, 14857]\n",
        "exc_neurons = [230, 16148]\n",
        "sad_neurons = [5810, 15539]\n",
        "fear_neurons = [7769]\n",
        "love_neuron = [2249, 4326, 5810, 15366]\n",
        "other_neurons = [1898, 3636, 7077, 16148]\n",
        "anger_neuron = [2438, 4560, 4859, 7579, 9065, 13324, 14857]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6bFJgyuxLXu"
      },
      "outputs": [],
      "source": [
        "ma = max_act_df.set_index('neuron').drop(columns=\"Unnamed: 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bVUBDJkxLXu"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "pol = defaultdict(list)\n",
        "texts = defaultdict(list)\n",
        "\n",
        "prompt = \"\"\"I can't believe that you said it to me:\"\"\"\n",
        "c = 0\n",
        "for n in other_neurons:\n",
        "    #max_act = find_max_activation(gemma, gemma_sae, activation_store, n, num_batches=50)\n",
        "    max_act = float(ma.loc[ma.index == n, 'max_activation'].iloc[0])\n",
        "\n",
        "    for _ in range(30):\n",
        "        #print(\"\\nWith anger neuron steering:\")\n",
        "        resp = generate_with_steering(gemma, gemma_sae, prompt, n, max_act, strength=3, max_new_tokens=20)\n",
        "        r = clean_and_shorten(resp)\n",
        "        pol[n].append(TextBlob(r).sentiment.polarity)\n",
        "        pol[n].append(TextBlob(r).sentiment.subjectivity)\n",
        "        pol[n].append(sia.polarity_scores(r))\n",
        "        texts[n].append(r)\n",
        "        sent = sentiment_pipeline(r)[0]\n",
        "        pol[n].append(sent['label'])\n",
        "        pol[n].append(sent['score'])\n",
        "        #print(clean_and_shorten(r), '\\n')\n",
        "    c += 1\n",
        "    print(c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBHWGAZixLXw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "combined = []\n",
        "\n",
        "for n in texts:\n",
        "    responses = texts[n]\n",
        "    for i, response in enumerate(responses):\n",
        "        base = 5 * i\n",
        "        record = {\n",
        "            'neuron': n,\n",
        "            'response_index': i,\n",
        "            'text': response,\n",
        "            'polarity': pol[n][base],              # TextBlob polarity\n",
        "            'subjectivity': pol[n][base + 1],      # TextBlob subjectivity\n",
        "            'label_sent': pol[n][base + 3],        # Transformer label\n",
        "            'score_sent': pol[n][base + 4],        # Transformer score\n",
        "        }\n",
        "\n",
        "        vader = pol[n][base + 2]\n",
        "        record.update(vader)\n",
        "\n",
        "        combined.append(record)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(combined)\n",
        "\n",
        "df.to_csv('other_max_3.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLTENb9WxLXw"
      },
      "outputs": [],
      "source": [
        "!pip install vaderSentiment"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "M2ZUfjKKYL8-",
        "rbJvOLvqfc-l",
        "Gb27R3yAYG4K",
        "qwi8F4Jsc7Qv"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMZr6qvaWpWJpmFSD4/PzD5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}