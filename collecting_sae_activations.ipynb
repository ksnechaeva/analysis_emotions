{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "93MYOGUj1M9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv8TVA2xog0Z"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sae_lens"
      ],
      "metadata": {
        "id": "jdsklyMEfk-E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XAhg2FPYi3Bn"
      },
      "outputs": [],
      "source": [
        "!pip install Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Pe0_M6i6oU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "ds = concatenate_datasets([ds['train'], ds['validation'], ds['test']])\n",
        "ds"
      ],
      "metadata": {
        "id": "zoeNCVrHva1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ds.features[\"labels\"].feature.names\n"
      ],
      "metadata": {
        "id": "GlKeT_Dq5vSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targeted_emotions = ['joy', 'anger', 'disgust', 'sadness', 'love', 'fear', 'excitement']\n",
        "labels = []\n",
        "for em in targeted_emotions:\n",
        "  labels.append(label_names.index(em))"
      ],
      "metadata": {
        "id": "QyudmgaT5xtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.filter(lambda x: any(label in labels for label in x['labels']))"
      ],
      "metadata": {
        "id": "_3Nrp7LF6bh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.filter(lambda x: len(x['labels']) == 1)"
      ],
      "metadata": {
        "id": "eyFJlUuj62p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "ids = np.load(\"ds_filt.npy\")"
      ],
      "metadata": {
        "id": "CRj2pKivh1Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_ds = ds.filter(lambda x: x[\"id\"] in ids)\n"
      ],
      "metadata": {
        "id": "x7DHauwXhyU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = filtered_ds"
      ],
      "metadata": {
        "id": "NmvnxoNfiCyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds)"
      ],
      "metadata": {
        "id": "QwO6y_KYgSpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYlBlf4klZMT"
      },
      "outputs": [],
      "source": [
        "ds = ds['train']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lIR7CltCcpG"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QSQMVqHfVrl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
        "#model = AutoModel.from_pretrained(\"google/gemma-2-2b\", output_hidden_states=True)\n",
        "#model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rgda5XxMtLsI"
      },
      "outputs": [],
      "source": [
        "!pip install sae_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK9GsCyOuSjO"
      },
      "outputs": [],
      "source": [
        "pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTFHTVjeueQa"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzKmq0GfsJ4-"
      },
      "outputs": [],
      "source": [
        "from sae_lens import (\n",
        "    SAE,\n",
        "    ActivationsStore,\n",
        "    HookedSAETransformer,\n",
        "    LanguageModelSAERunnerConfig,\n",
        "    SAEConfig,\n",
        "    SAETrainingRunner,\n",
        "    upload_saes_to_huggingface,\n",
        ")\n",
        "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
        "#from sae_vis import SaeVisConfig, SaeVisData, SaeVisLayoutConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fgzvqP8KsJ7V"
      },
      "outputs": [],
      "source": [
        "def format_value(value):\n",
        "    return \"{{{0!r}: {1!r}, ...}}\".format(*next(iter(value.items()))) if isinstance(value, dict) else repr(value)\n",
        "\n",
        "\n",
        "release = get_pretrained_saes_directory()[\"gemma-scope-2b-pt-res\"]\n",
        "\n",
        "print(\n",
        "    tabulate(\n",
        "        [[k, format_value(v)] for k, v in release.__dict__.items()],\n",
        "        headers=[\"Field\", \"Value\"],\n",
        "        tablefmt=\"simple_outline\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0vFegKAwj1PU"
      },
      "outputs": [],
      "source": [
        "data = [[id, path, release.neuronpedia_id[id]] for id, path in release.saes_map.items()]\n",
        "\n",
        "print(\n",
        "    tabulate(\n",
        "        data,\n",
        "        headers=[\"SAE id\", \"SAE path (HuggingFace)\", \"Neuronpedia ID\"],\n",
        "        tablefmt=\"simple_outline\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting activations"
      ],
      "metadata": {
        "id": "jPEcqKIJ0-P3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6XgKVRhBZkO"
      },
      "outputs": [],
      "source": [
        "texts = [item[\"text\"] for item in ds]\n",
        "labels = [item[\"labels\"] for item in ds]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngCkucpKzw4s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sae_lens import SAE, ActivationsStore\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "gemma_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=\"layer_20/width_16k/average_l0_71\",\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "gemma = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PS-gA8Yoacrd"
      },
      "outputs": [],
      "source": [
        "label_names = load_dataset(\"go_emotions\", \"simplified\", split=\"train\").features[\"labels\"].feature.names\n",
        "#sad_id = label_names.index(\"sadness\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4LLogJg1BgP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIoQaJCbKEkR"
      },
      "outputs": [],
      "source": [
        "targeted_emotions = ['joy', 'anger', 'disgust', 'sadness']\n",
        "labels = []\n",
        "for em in targeted_emotions:\n",
        "  labels.append(label_names.index(em))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6_J_Py61Hbm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpHrfny4KZOE"
      },
      "outputs": [],
      "source": [
        "ds_reduced = ds.filter(lambda x: any(label in labels for label in x['labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juoxfnMLcf5F"
      },
      "outputs": [],
      "source": [
        "texts = [item['text'] for item in ds_reduced ]\n",
        "labels = [item['labels'] for item in ds_reduced ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUHc8HsDobn6"
      },
      "outputs": [],
      "source": [
        "#just to measure length\n",
        "lengths = [len(tokenizer.encode(text)) for text in texts]\n",
        "max_length = max(lengths)\n",
        "print(f\"Max tokenized length across all samples: {max_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjSh599m0ziL"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "batch_size = 4\n",
        "max_length = 52\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
        "    try:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Get batch\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        # Tokenize\n",
        "        tokenized = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"].to(device)\n",
        "\n",
        "        # Forward pass with SAE and cache activations\n",
        "        _, cache = gemma.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[gemma_sae],\n",
        "            stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"],\n",
        "        )\n",
        "\n",
        "        # SAE activations (features)\n",
        "        sae_acts = cache[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"]  # [B, T, F]\n",
        "        final_acts = sae_acts[:, -1, :].detach().cpu()\n",
        "\n",
        "        # Sparsity = number of active features\n",
        "        sparsity = (sae_acts[:, -1, :] > 1).sum(dim=-1)  # [B]\n",
        "\n",
        "        decoded_tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in input_ids]\n",
        "\n",
        "\n",
        "        for i in range(len(batch_texts)):\n",
        "            results.append({\n",
        "                \"input_ids\": input_ids[i].detach().cpu(),              # torch.Tensor\n",
        "                \"tokens\": decoded_tokens[i],                          # list of strings\n",
        "                \"activation\": final_acts[i],                          # torch.Tensor\n",
        "                \"sparsity\": int(sparsity[i]),                         # int\n",
        "            })\n",
        "\n",
        "        # Cleanup\n",
        "        del cache, sae_acts, final_acts, sparsity, input_ids, tokenized\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"⚠️ OOM on batch {i}-{i+batch_size}: {e}\")\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6PhNtPfMnS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"sae_results_filtered.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_AzIbv1hl5a"
      },
      "outputs": [],
      "source": [
        "print(\"Average sparsity:\", torch.tensor([r[\"sparsity\"] for r in results]).float().mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUrS0dpAh0iG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sparsities = [r[\"sparsity\"] for r in results]\n",
        "plt.hist(sparsities, bins=50)\n",
        "plt.title(\"Sparsity Distribution\")\n",
        "plt.xlabel(\"# of active SAE features (>1)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV2MIV-WnGYT"
      },
      "outputs": [],
      "source": [
        "topk = results[0][\"activation\"].topk(10)\n",
        "\n",
        "top_neurons = topk.indices.tolist()\n",
        "top_values = topk.values.tolist()\n",
        "\n",
        "print(f\"Top firing SAE neurons and their activations for input:{results[0]['text']}\")\n",
        "for idx, val in zip(top_neurons, top_values):\n",
        "    print(f\"Neuron {idx} ➝ Activation: {val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kNdA0SShdAiF"
      },
      "outputs": [],
      "source": [
        "inds = []\n",
        "\n",
        "for i, entry in enumerate(results):\n",
        "    text = entry[\"text\"]\n",
        "    activation = entry[\"activation\"]\n",
        "\n",
        "    val, idx = activation.max(-1)\n",
        "    inds.append(idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUbQGVyFaUVJ"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import torch\n",
        "\n",
        "sample = results[0]\n",
        "\n",
        "activation = sample[\"activation\"]\n",
        "activation_np = activation.numpy()\n",
        "\n",
        "px.line(\n",
        "    y=activation_np,\n",
        "    title=f\"SAE Activations for Sample 0 — Final Token\",\n",
        "    labels={\"index\": \"Neuron (Latent Feature)\", \"value\": \"Activation\"},\n",
        "    width=1000\n",
        ").update_layout(showlegend=False).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE5Ms_Q774cM"
      },
      "source": [
        "# Drafts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47ftxgf0R8XN"
      },
      "outputs": [],
      "source": [
        "results = [\n",
        "    {\n",
        "        \"text\": decoded_prompts[i],\n",
        "        \"activation\": final_acts[i].detach().cpu()\n",
        "    }\n",
        "    for i in range(len(texts))\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y7wwrtCEU9N"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "inappropriate as it will concatenate differently labeled text into one chunk\n",
        "\n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "\n",
        "token_dataset = tokenize_and_concatenate(\n",
        "    dataset=ds,  # type: ignore\n",
        "    tokenizer=tokenizer,  # type: ignore\n",
        "    streaming=True,\n",
        "    max_length=gemma_sae.cfg.context_size,\n",
        "    add_bos_token=gemma_sae.cfg.prepend_bos,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hoSneENCxic"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sae_lens import SAE, ActivationsStore\n",
        "from transformer_lens.utils import tokenize_and_concatenate\n",
        "from transformers import AutoTokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "'''\n",
        "tokenized_ds = tokenize_and_concatenate(\n",
        "    dataset=ds.remove_columns([col for col in ds.column_names if col != \"text\"]),\n",
        "    tokenizer=tokenizer,\n",
        "    column_name=\"text\",\n",
        "    streaming=False,\n",
        "    max_length=gemma_sae.cfg.context_size,\n",
        "    add_bos_token=gemma_sae.cfg.prepend_bos,\n",
        ")\n",
        "'''\n",
        "\n",
        "# Collect activations\n",
        "batch_size = 2\n",
        "all_acts, all_labels = [], []\n",
        "\n",
        "for i in range(0, len(tokenized_ds), batch_size):\n",
        "    batch = tokenized_ds.select(range(i, min(i + batch_size, len(tokenized_ds))))\n",
        "    tokens = batch[\"tokens\"]  # ✅ This is now a list of LongTensors\n",
        "    input_ids = pad_sequence(tokens, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
        "\n",
        "    try:\n",
        "        _, cache = gemma.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[gemma_sae],\n",
        "            stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "        )\n",
        "        sae_acts = cache[f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        final_acts = sae_acts[:, -1, :].detach().cpu()\n",
        "\n",
        "        all_acts.append(final_acts)\n",
        "        all_labels.extend(labels[i:i + len(tokens)])\n",
        "\n",
        "        del cache, sae_acts, final_acts, input_ids\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Skipping batch {i}-{i+batch_size} due to OOM: {e}\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me4YcWA-Mpog"
      },
      "outputs": [],
      "source": [
        "pip install ace_tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57E10fhKFSw"
      },
      "outputs": [],
      "source": [
        "X = torch.cat(all_acts, dim=0)\n",
        "y = all_labels\n",
        "\n",
        "label_to_acts = defaultdict(list)\n",
        "for xi, yi in zip(X, y):\n",
        "    for label in yi:\n",
        "        label_to_acts[label].append(xi)\n",
        "\n",
        "label_to_mean = {label: torch.stack(acts).mean(dim=0) for label, acts in label_to_acts.items()}\n",
        "\n",
        "# Output as DataFrame\n",
        "import pandas as pd\n",
        "df_mean = pd.DataFrame.from_dict({k: v.numpy() for k, v in label_to_mean.items()}, orient=\"index\")\n",
        "\n",
        "display(df_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dj039GVef0V"
      },
      "outputs": [],
      "source": [
        "top_k = 10\n",
        "emotion_to_top_neurons = {}\n",
        "\n",
        "for label, mean_acts in label_to_mean.items():\n",
        "    top_values, top_indices = mean_acts.topk(top_k)\n",
        "    emotion_to_top_neurons[label] = list(zip(top_indices.tolist(), top_values.tolist()))\n",
        "\n",
        "# Convert to readable DataFrame\n",
        "emotion_top_df = pd.DataFrame.from_dict(emotion_to_top_neurons, orient=\"index\")\n",
        "emotion_top_df.columns = [f\"TOP {i+1} Neuron\" for i in range(top_k)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq2-k-cDeho2"
      },
      "outputs": [],
      "source": [
        "emotion_top_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myP_u0-gdouS"
      },
      "outputs": [],
      "source": [
        "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
        "\n",
        "# this function should open\n",
        "neuronpedia_quick_list = get_neuronpedia_quick_list(gemma_sae, test_feature_idx_gpt)\n",
        "\n",
        "if COLAB:\n",
        "    # If you're on colab, click the link below\n",
        "    print(neuronpedia_quick_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUYZ1IA_J_El"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_to_acts = defaultdict(list)\n",
        "for xi, yi in zip(X, y):\n",
        "    label_to_acts[yi].append(xi)\n",
        "\n",
        "label_to_mean = {label: torch.stack(acts).mean(dim=0) for label, acts in label_to_acts.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwnn6KapJ_HK"
      },
      "outputs": [],
      "source": [
        "from sae_lens.analysis.neuronpedia_integration import\n",
        "# Example: interpret neuron 1234 from your SAE\n",
        "neuron_id = 1234\n",
        "interpretation = neuronpedia.analyze_feature(\n",
        "    feature_idx=neuron_id,\n",
        "    sae=gemma_sae,\n",
        "    model=gemma,\n",
        "    top_k=15  # get top associated tokens or completions\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh5_aArcJ_JY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Pb1sO7HJ_MG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMaajGcXJ_OM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWJLJOQzJ_Q2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6yKKoTQJ_TI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiN2WqiF3vhA"
      },
      "outputs": [],
      "source": [
        "for name, param in cache.items():\n",
        "    if \"hook_sae\" in name:\n",
        "        print(f\"{name:<43}: {tuple(param.shape)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPquSQ7h4PYl"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRT_rJ3K4C5h"
      },
      "outputs": [],
      "source": [
        "# Plot line chart of latent activations\n",
        "px.line(\n",
        "    sae_acts_post.cpu().numpy(),\n",
        "    title=f\"Latent activations at the final token position ({sae_acts_post.nonzero().numel()} alive)\",\n",
        "    labels={\"index\": \"Latent\", \"value\": \"Activation\"},\n",
        "    width=1000,\n",
        ").update_layout(showlegend=False).show()\n",
        "\n",
        "# Print the top 5 latents, and inspect their dashboards\n",
        "for act, ind in zip(*sae_acts_post.topk(3)):\n",
        "    print(f\"Latent {ind} had activation {act:.2f}\")\n",
        "    display_dashboard(latent_idx=ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu3oxLjq8f4d"
      },
      "outputs": [],
      "source": [
        "sae_acts_post_hook_name = f\"{gemma_sae.cfg.hook_name}.hook_sae_acts_post\"\n",
        "all_positive_acts = []\n",
        "\n",
        "for i in tqdm(range(total_batches)):\n",
        "        tokens = act_store.get_batch_tokens()\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            tokens,\n",
        "            saes=[gemma_sae],\n",
        "            stop_at_layer=gemma_sae.cfg.hook_layer + 1,\n",
        "            names_filter=[sae_acts_post_hook_name],\n",
        "        )\n",
        "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
        "        all_positive_acts.extend(acts[acts > 0].cpu().tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk3TV8_GaapT"
      },
      "source": [
        "### Neuronopedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiDgfZbqmsye"
      },
      "outputs": [],
      "source": [
        "results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF9Shkv8iqKG"
      },
      "outputs": [],
      "source": [
        "sae_release = \"gemma-2-2b\"\n",
        "sae_id = \"19-gemmascope-res-16k\"\n",
        "feature_idx = 11882  # example neuron\n",
        "\n",
        "url = f\"https://neuronpedia.org/{sae_release}/{sae_id}/{feature_idx}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5gQozaIjI5q"
      },
      "outputs": [],
      "source": [
        "for neuron in top_neurons:\n",
        "    print(f\"Neuron {neuron} ➝ https://neuronpedia.org/gemma-2-2b/19-gemmascope-res-16k__l0-137/2725\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYKgwaYIiqMB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(\"https://neuronpedia.org/gemma-2-2b/19-gemmascope-res-16k__l0-137/2725\", width=1200, height=600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ehP3IiwiqOb"
      },
      "outputs": [],
      "source": [
        "Top firing SAE neurons and their activations for input:I miss them being alive\n",
        "Neuron 15509 ➝ Activation: 47.4543\n",
        "Neuron 4326 ➝ Activation: 35.8033\n",
        "Neuron 14232 ➝ Activation: 34.3398\n",
        "Neuron 204 ➝ Activation: 29.5415\n",
        "Neuron 15328 ➝ Activation: 27.9000\n",
        "Neuron 11864 ➝ Activation: 26.3788\n",
        "Neuron 1692 ➝ Activation: 26.2759\n",
        "Neuron 9768 ➝ Activation: 25.3538\n",
        "Neuron 15539 ➝ Activation: 23.9061\n",
        "Neuron 14084 ➝ Activation: 23.6004"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQe-EGY9iqQn"
      },
      "outputs": [],
      "source": [
        "# for layer 20 from gemma scope tutorial\n",
        "\n",
        "\n",
        "from IPython.display import IFrame\n",
        "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
        "\n",
        "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
        "    return html_template.format(sae_release, sae_id, feature_idx)\n",
        "\n",
        "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=15539)\n",
        "IFrame(html, width=1200, height=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O902JzetTe3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AV758AHTe5Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETK8cwEXTe7f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qic-DbZITe9j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL1k2dcliqS7"
      },
      "outputs": [],
      "source": [
        "def fetch_max_activating_examples(\n",
        "    model: HookedSAETransformer,\n",
        "    sae: SAE,\n",
        "    act_store: ActivationsStore,\n",
        "    latent_idx: int,\n",
        "    total_batches: int = 100,\n",
        "    k: int = 10,\n",
        "    buffer: int = 10,\n",
        "    display: bool = False,\n",
        ") -> list[tuple[float, list[str], int]]:\n",
        "    \"\"\"\n",
        "    Displays the max activating examples across a number of batches from the\n",
        "    activations store, using the `display_top_seqs` function.\n",
        "    \"\"\"\n",
        "    sae_acts_post_hook_name = f\"{sae.cfg.hook_name}.hook_sae_acts_post\"\n",
        "\n",
        "    # Create list to store the top k activations for each batch. Once we're done,\n",
        "    # we'll filter this to only contain the top k over all batches\n",
        "    data = []\n",
        "\n",
        "    for _ in tqdm(range(total_batches)):\n",
        "        tokens = act_store.get_batch_tokens()\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            tokens,\n",
        "            saes=[sae],\n",
        "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
        "            names_filter=[sae_acts_post_hook_name],\n",
        "        )\n",
        "        acts = cache[sae_acts_post_hook_name][..., latent_idx]\n",
        "\n",
        "        # Get largest indices, get the corresponding max acts, and get the surrounding indices\n",
        "        k_largest_indices = get_k_largest_indices(acts, k=k, buffer=buffer)\n",
        "        tokens_with_buffer = index_with_buffer(tokens, k_largest_indices, buffer=buffer)\n",
        "        str_toks = [model.to_str_tokens(toks) for toks in tokens_with_buffer]\n",
        "        top_acts = index_with_buffer(acts, k_largest_indices).tolist()\n",
        "        data.extend(list(zip(top_acts, str_toks, [buffer] * len(str_toks))))\n",
        "\n",
        "    data = sorted(data, key=lambda x: x[0], reverse=True)[:k]\n",
        "    if display:\n",
        "        display_top_seqs(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Display your results, and also test them\n",
        "buffer = 10\n",
        "data = fetch_max_activating_examples(gpt2, gpt2_sae, gpt2_act_store, latent_idx=9, buffer=buffer, k=5, display=True)\n",
        "first_seq_str_tokens = data[0][1]\n",
        "assert first_seq_str_tokens[buffer] == \" new\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "7-OOphHJaUmz",
        "Uk3TV8_GaapT",
        "nE5Ms_Q774cM"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyORFNqq4hyBkcxkdpgCy5Vf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}