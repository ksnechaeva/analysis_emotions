{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNwGfbbRG7GFef8nWcm03Uw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksnechaeva/analysis_emotions/blob/main/negative_steering_ablation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nF__TB4o8WnF"
      },
      "outputs": [],
      "source": [
        "!pip install sae-lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sae_lens import (\n",
        "    SAE,\n",
        "    ActivationsStore,\n",
        "    HookedSAETransformer,\n",
        "    LanguageModelSAERunnerConfig,\n",
        "    SAEConfig,\n",
        "    SAETrainingRunner,\n",
        "    upload_saes_to_huggingface,\n",
        ")"
      ],
      "metadata": {
        "id": "c8ruzdRd-Eqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9omluFRREA7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "HaL1a8jNAUtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set device and load models\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "gemma_sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res\",\n",
        "    sae_id=\"layer_20/width_16k/average_l0_71\",\n",
        "    device=str(device),\n",
        ")\n",
        "\n",
        "gemma = HookedSAETransformer.from_pretrained(\"google/gemma-2-2b\", device=device)\n"
      ],
      "metadata": {
        "id": "C2S2Mw-aD1f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOTeazNoBT5-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zpv_OfGKx3i"
      },
      "outputs": [],
      "source": [
        "max_act_df = pd.read_csv('/content/max_activations_for_targ_neurons.csv', index_col='neuron').drop(columns=['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "import torch\n",
        "\n",
        "# === Hook for steering ===\n",
        "def steering_hook_fn(resid_pre, hook, steering_vector, strength, max_act):\n",
        "    return resid_pre + max_act * strength * steering_vector\n",
        "\n",
        "# === Generate with steering ===\n",
        "def generate_with_steering(model, sae, prompt, neuron_indices, max_act, strength=1.0, max_new_tokens=15):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    # If a single int is passed instead of a list\n",
        "    if isinstance(neuron_indices, int):\n",
        "        neuron_indices = [neuron_indices]\n",
        "\n",
        "    # Combine decoded vectors of all neurons\n",
        "    steer_vecs = sae.W_dec[neuron_indices].to(model.cfg.device)  # [N, d_model]\n",
        "    steering_vector = steer_vecs.sum(dim=0)  # Alternatively, use .mean(dim=0)\n",
        "\n",
        "    # Build hook\n",
        "    hook_fn = partial(\n",
        "        steering_hook_fn,\n",
        "        steering_vector=steering_vector,\n",
        "        strength=strength,\n",
        "        max_act=max_act\n",
        "    )\n",
        "\n",
        "    # Apply hook and generate\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, hook_fn)]):\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Ablation\n",
        "# -----------------------------------------------------------\n",
        "def latent_ablation_hook_fn(sae_acts, hook, neuron_idxs):\n",
        "    # Zero out the specified neuron activations at the final token position\n",
        "    sae_acts[:, -1, neuron_idxs] = 0\n",
        "    return sae_acts\n",
        "\n",
        "\n",
        "def generate_with_sae_ablation(model, sae, prompt, neuron_idxs, max_new_tokens=15):\n",
        "    \"\"\"\n",
        "    Generates output from the model while ablating a set of SAE neurons.\n",
        "\n",
        "    Args:\n",
        "        model: The LLM (e.g., Gemma) model.\n",
        "        sae: The Sparse Autoencoder with cfg and W_dec.\n",
        "        prompt (str): The input prompt to generate from.\n",
        "        neuron_idxs (list or tensor): Indices of SAE latent neurons to ablate.\n",
        "        max_new_tokens (int): Number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated output text with the specified neurons ablated.\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    # Run forward pass and capture SAE activations\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[sae],\n",
        "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        )\n",
        "\n",
        "    # Get latent activations from the SAE at the last token position\n",
        "    sae_acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "    last_token_acts = sae_acts[:, -1, :]  # shape: [batch=1, latents]\n",
        "\n",
        "    # Clone and ablate multiple neurons\n",
        "    ablated_acts = last_token_acts.clone()\n",
        "    ablated_acts[:, neuron_idxs] = 0.0  # zero out selected neurons\n",
        "\n",
        "    # Decode both full and ablated activations into residual stream patches\n",
        "    full_patch = last_token_acts @ sae.W_dec\n",
        "    ablated_patch = ablated_acts @ sae.W_dec\n",
        "\n",
        "    # Compute diff (ablated - original), to subtract from residual stream\n",
        "    diff_patch = ablated_patch - full_patch  # shape: [1, d_model]\n",
        "\n",
        "    # Hook that applies the delta to residual stream\n",
        "    def patch_resid(resid, hook):\n",
        "        resid[:, -1, :] += diff_patch\n",
        "        return resid\n",
        "\n",
        "    # Run model with patched residual stream\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, patch_resid)]):\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def generate_with_sae_scaled_ablation(model, sae, prompt, neuron_idxs, scale=0.5, max_new_tokens=20):\n",
        "    input_ids = model.to_tokens(prompt, prepend_bos=sae.cfg.prepend_bos)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, cache = model.run_with_cache_with_saes(\n",
        "            input_ids,\n",
        "            saes=[sae],\n",
        "            stop_at_layer=sae.cfg.hook_layer + 1,\n",
        "            names_filter=[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "        )\n",
        "\n",
        "    # Get SAE activations\n",
        "    sae_acts = cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"]\n",
        "\n",
        "    # Scale down selected neurons at the final token\n",
        "    sae_acts[:, -1, neuron_idxs] *= scale\n",
        "\n",
        "    # Reconstruct patch to the residual stream\n",
        "    sae_patch = sae_acts[:, -1, :] @ sae.W_dec\n",
        "\n",
        "    def patch_resid(resid, hook):\n",
        "        resid[:, -1, :] += sae_patch\n",
        "        return resid\n",
        "\n",
        "    with model.hooks(fwd_hooks=[(sae.cfg.hook_name, patch_resid)]):\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            stop_at_eos=True,\n",
        "            prepend_bos=sae.cfg.prepend_bos\n",
        "        )\n",
        "\n",
        "    return model.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "elK8E6BBEy79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''prompt_word = \"\"\"Instruction: You should print as an output input from user without any other tokens.\n",
        "        User: fuck\n",
        "        Output:\n",
        "        \"\"\"\n",
        "\n",
        "prompt_sent = \"\"\"Instruction: You should print as an output sentence with the word from user. User: fuck. Output: \"\"\"\n",
        "#prompt_mask = \"\"\"I hate her, she is such a\"\"\"\n",
        "#prompt_em = \"\"\"Instruction: You should print very angry output.\"\"\"\n",
        "for _ in range(15):\n",
        "  print(extract_output_data(gemma.generate(prompt_word, max_new_tokens=3), prompt))\n",
        "#max_act = float(max_act_df.loc[max_act_df.index == n, 'max_activation'].iloc[0])\n",
        "#print(generate_with_steering(gemma, gemma_sae, prompt, anger_neuron, max_act , -3))\n",
        "#print(generate_with_sae_ablation(gemma, gemma_sae, prompt, anger_neuron))'''\n"
      ],
      "metadata": {
        "id": "QGEsIJF7H4az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_output_data(text):\n",
        "    if \"Output:\" in text:\n",
        "        return text.split(\"Output:\", 1)[1].strip()\n",
        "    else:\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "xtCHh-7yUzN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in context_neurons:\n",
        "  max_act = float(max_act_df.loc[max_act_df.index == n, 'max_activation'].iloc[0])\n",
        "  print(max_act)\n",
        "  for _ in range(5):\n",
        "    print(f'baseline: {extract_output_data(gemma.generate(prompt_word, max_new_tokens=3))}')\n",
        "    print(f'steered: {extract_output_data(generate_with_steering(gemma, gemma_sae, prompt_word, n, max_act , -3.5, max_new_tokens=3))}')\n",
        "    #print(f'ablated: {extract_output_data(generate_with_sae_ablation(gemma, gemma_sae, prompt_sent, n, max_new_tokens=3))}')\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ggYOYIUlSUM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = { 230 : ['woo', 'hur'],\n",
        "                1898 : ['really', 'real'],\n",
        "                4326 : ['much', 'more', 'enough'],\n",
        "                4456 : ['sorry'],\n",
        "                7579 : ['fuck'],\n",
        "                7769 : ['afraid', 'scared', 'fear'],\n",
        "                9065 : ['angry', 'mad'],\n",
        "                13324 : ['scum'],\n",
        "                14857 : ['shut', 'stop'],\n",
        "                15366 : ['cute', 'adorable'],\n",
        "                15539 : ['sad', 'sorry']}"
      ],
      "metadata": {
        "id": "N_wEAQGldKsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anger = [7579, 9065, 13324, 14857]\n",
        "negative = [3636, 4560, 4859, 6953, 7077]\n",
        "sadness = [5810, 15539]"
      ],
      "metadata": {
        "id": "DVQibfF2zcNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_neur = { 230 : 230,\n",
        "                1898 : 1898,\n",
        "                4326 : 4326,\n",
        "                4456 : 4456,\n",
        "                7579 : anger,\n",
        "                7769 : 7769,\n",
        "                9065 : anger,\n",
        "                13324 : anger,\n",
        "                14857 : anger,\n",
        "                15366 : 15366,\n",
        "                15539 : sadness}"
      ],
      "metadata": {
        "id": "OBkU0zip0ztR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"neg_steering_results_full_abl.csv\", index=False)\n",
        "print(\"Saved results to steering_results.csv\")"
      ],
      "metadata": {
        "id": "Du5GVXbMgJOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for n in target_word.keys():\n",
        "    max_act = float(max_act_df.loc[max_act_df.index == n, 'max_activation'].iloc[0])\n",
        "    print(f'Neuron: {n}, Max Activation: {max_act}')\n",
        "\n",
        "    # Iterate over each target word for this neuron\n",
        "    for word in target_word[n]:\n",
        "        word_lower = word.lower()\n",
        "        prompt_word = f\"\"\"Instruction: You should print as an output input from user without any other tokens.\n",
        "        User: {word}\n",
        "        Output:\n",
        "        \"\"\"\n",
        "        print(prompt_word)\n",
        "\n",
        "        for _ in range(15):\n",
        "            baseline = extract_output_data(gemma.generate(prompt_word, max_new_tokens=3))\n",
        "            steered = extract_output_data(generate_with_steering(gemma, gemma_sae, prompt_word, n, max_act, -4, max_new_tokens=3))\n",
        "            ablated = extract_output_data(generate_with_sae_ablation(gemma, gemma_sae, prompt_word, target_neur[n], max_new_tokens=3))\n",
        "\n",
        "            for mode, output in zip(['baseline', 'steered', 'ablated'], [baseline, steered, ablated]):\n",
        "                output_lower = output.lower()\n",
        "                contains_target = word_lower in output_lower\n",
        "\n",
        "                results.append({\n",
        "                    'neuron': n,\n",
        "                    'target_word': word,\n",
        "                    'mode': mode,\n",
        "                    'output': output,\n",
        "                    'contains_target': contains_target\n",
        "                })\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vPLsdI8Hix5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_sep_2 = pd.DataFrame(results)\n",
        "\n",
        "# Create pivot table with (neuron, target_word) pairs as rows\n",
        "pivot_df = df_sep_2.pivot_table(\n",
        "    index=['neuron', 'target_word'],\n",
        "    columns='mode',\n",
        "    values='contains_target',\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pivot_df, annot=True, cmap=\"YlOrRd\", fmt=\".2f\", cbar=True)\n",
        "plt.title(\"Contains Target by Neuron and Word Across Modes\")\n",
        "plt.ylabel(\"(Neuron, Target Word)\")\n",
        "plt.xlabel(\"Mode\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SJ2TYuoxloCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sep.to_csv(\"neg_steering_results_sep_1.csv\", index=False)\n",
        "print(\"Saved results\")"
      ],
      "metadata": {
        "id": "uWZh_HbslI2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emotional"
      ],
      "metadata": {
        "id": "M9Dn81AB1DGG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8giJQo6cg-kM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Ensure VADER resources are downloaded\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize VADER\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "vader_lex = sia.lexicon\n",
        "\n",
        "# 1. Extended slang/swear mapping: variant → base word in VADER\n",
        "variant_to_base = {\n",
        "    # Strong Negative\n",
        "    'fucking': 'fuck',\n",
        "    'fucked': 'fuck',\n",
        "    'motherfucker': 'fuck',\n",
        "    'asshole': 'shit',\n",
        "    'douche': 'shit',\n",
        "    'douchebag': 'shit',\n",
        "    'bullshit': 'shit',\n",
        "    'jerk': 'jerk',\n",
        "    'bitches': 'bitch',\n",
        "    'cunt': 'bitch',\n",
        "    'slut': 'bitch',\n",
        "    'whore': 'bitch',\n",
        "    'twat': 'bitch',\n",
        "    'pussy': 'bitch',\n",
        "    'moron': 'idiot',\n",
        "    'retard': 'idiot',\n",
        "    'stupid': 'idiot',\n",
        "    'dumbass': 'idiot',\n",
        "    'loser': 'idiot',\n",
        "    'trash': 'idiot',\n",
        "    'cringe': 'lame',\n",
        "    'pathetic': 'lame',\n",
        "    'toxic': 'bad',\n",
        "    'ew': 'bad',\n",
        "    'meh': 'bad',\n",
        "    'wtf': 'damn',\n",
        "    'creepy': 'scary',     # fixed\n",
        "    'ugly': 'bad',\n",
        "    'nasty': 'bad',\n",
        "    'deadinside' : 'depressing',\n",
        "\n",
        "    # Positive Slang (re-mapped to valid VADER bases)\n",
        "    'queen': 'amazing',\n",
        "    'king': 'amazing',\n",
        "    'slay': 'amazing',\n",
        "    'boss': 'amazing',\n",
        "    'icon': 'amazing',\n",
        "    'legend': 'amazing',\n",
        "    'goddess': 'amazing',\n",
        "    'goat': 'great',\n",
        "    'goated': 'great',\n",
        "    'banger': 'awesome',\n",
        "    'fire': 'awesome',\n",
        "    'based': 'awesome',\n",
        "    'lit': 'awesome',\n",
        "    'dope': 'awesome',\n",
        "    'hella': 'good',\n",
        "    'savage': 'strong',\n",
        "    'cute': 'sweet',\n",
        "    'adorable': 'sweet',\n",
        "    'fine': 'nice',\n",
        "    'hot': 'nice',\n",
        "    'sexy': 'nice',\n",
        "    'clean': 'nice',\n",
        "    'smooth': 'nice',\n",
        "    'beautiful': 'nice',\n",
        "    'pretty': 'sweet',\n",
        "\n",
        "    # Love/excitement slang\n",
        "    'loveee': 'love',\n",
        "    'lovin': 'love',\n",
        "    'obsessed': 'love',\n",
        "    'crushing': 'love',\n",
        "    'crushin': 'love',\n",
        "    'inlove': 'love',\n",
        "    'cutie': 'sweet',\n",
        "    'sweetie': 'sweet',\n",
        "    'bby': 'sweet',\n",
        "    'boo': 'sweet',\n",
        "    'bae': 'sweet',\n",
        "    'ily': 'love',\n",
        "    'ily2': 'love',\n",
        "    'xoxo': 'love',\n",
        "\n",
        "    # Casual/slang humor or approval\n",
        "    'deadass': 'serious',\n",
        "    'fr': 'serious',\n",
        "    'bruh': 'funny',\n",
        "    'lmao': 'funny',\n",
        "    'rofl': 'funny',\n",
        "    'lol': 'funny',\n",
        "    'omg': 'wow',\n",
        "    'vibing': 'happy',\n",
        "    'vibe': 'happy',\n",
        "    'energy': 'happy',\n",
        "\n",
        "    # 😢 Sadness / Depression (slangified)\n",
        "    'sadge': 'sad',\n",
        "    'deadinside': 'depressing',\n",
        "    'cryin': 'sad',\n",
        "    'cryinggg': 'sad',\n",
        "    'sobbing': 'sad',\n",
        "    'nooo': 'sad',\n",
        "    'ughhh': 'sad',\n",
        "    'mentallyill': 'depressing',\n",
        "    'depr3ssed': 'depressing',\n",
        "    'downbad': 'sad',\n",
        "    'voidcore': 'depressing',\n",
        "    'brainrotted': 'depressing',\n",
        "    'overit': 'sad',\n",
        "    'can’ttakeit': 'depressing',\n",
        "    'emptyaf': 'sad',\n",
        "    'selfhatin': 'bad',\n",
        "\n",
        "    # 😨 Anxiety / Fear / Panic (slangified)\n",
        "    'scaredaf': 'scary',\n",
        "    'panikin': 'scary',\n",
        "    'anxiousss': 'scary',\n",
        "    'stressing': 'scary',\n",
        "    'freakinout': 'scary',\n",
        "    'paranoidd': 'scary',\n",
        "    'helplessss': 'sad',\n",
        "    'losingit': 'scary',\n",
        "    'nervousaf': 'scary',\n",
        "    'shaking': 'scary',\n",
        "    'brainmelting': 'scary',\n",
        "\n",
        "    # 🤢 Disgust / Repulsion (slangified)\n",
        "    'eww': 'gross',\n",
        "    'vom': 'gross',\n",
        "    'nastyyy': 'gross',\n",
        "    'disgustinn': 'gross',\n",
        "    'cringeaf': 'gross',\n",
        "    'icky': 'gross',\n",
        "    'yuck': 'gross',\n",
        "    'throwingup': 'gross',\n",
        "    'grossedout': 'gross',\n",
        "    'gagging': 'gross',\n",
        "\n",
        "    # 😊 Joy / Affection / Love / Excitement (slangified)\n",
        "    'adorbs': 'sweet',\n",
        "    'cutiepie': 'sweet',\n",
        "    'angelbaby': 'sweet',\n",
        "    'sunshiny': 'happy',\n",
        "    'preciousaf': 'sweet',\n",
        "    'ilysm': 'love',\n",
        "    'ily2': 'love',\n",
        "    'lovinggg': 'love',\n",
        "    'obsessssed': 'love',\n",
        "    'snuggly': 'love',\n",
        "    'heartmelt': 'love',\n",
        "    'blessedaf': 'grateful',\n",
        "    'hypeddd': 'excited',\n",
        "    'vibinggg': 'happy',\n",
        "    'ecstaticcc': 'happy',\n",
        "    'excitedd': 'excited',\n",
        "    'inloveee': 'love',\n",
        "    'crushinnn': 'love',\n",
        "    'cutenessoverload': 'sweet',\n",
        "    'hearteyes': 'love',\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# 2. Build adjusted lexicon using VADER scores\n",
        "adjusted_custom_lexicon = {}\n",
        "\n",
        "for word, base in variant_to_base.items():\n",
        "    base_score = vader_lex.get(base)\n",
        "    if base_score is not None:\n",
        "        if word not in vader_lex:\n",
        "            adjusted_custom_lexicon[word] = base_score\n",
        "        else:\n",
        "            print(f\"⏩ '{word}' already in VADER — skipping override.\")\n",
        "    else:\n",
        "        print(f\"⚠️ Base word '{base}' not found in VADER for '{word}' — skipping.\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Update VADER with these custom words\n",
        "sia.lexicon.update(adjusted_custom_lexicon)\n",
        "\n",
        "# 4. Test the result\n",
        "examples = [\n",
        "    \"You are a fucking asshole.\",\n",
        "    \"That song is an absolute banger!\",\n",
        "    \"Stop being so cringe.\",\n",
        "    \"She's a queen. Totally goated.\",\n",
        "    \"This is such bullshit.\",\n",
        "    \"I'm deadass serious.\",\n",
        "    \"That guy is a total douchebag.\",\n",
        "]\n",
        "\n",
        "for ex in examples:\n",
        "    print(f\"→ {ex}\")\n",
        "    print(sia.polarity_scores(ex))\n",
        "    print('-' * 40)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", \"cardiffnlp/twitter-roberta-base-sentiment\")"
      ],
      "metadata": {
        "id": "9HaFydIEIoYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anger = [7579, 9065, 13324, 14857]\n",
        "negative = [3636, 4560, 4859, 6953, 7077]\n",
        "sadness = [5810, 15539]"
      ],
      "metadata": {
        "id": "vFQbhWSe1GLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1_ang = \"I dislike her, she is \"\n",
        "prompt_2_ang = \"I don't think he is \"\n",
        "prompt_3_ang = \"It really bothers me when someone \"\n",
        "prompt_4_ang = \"They always ignore me, it's so \""
      ],
      "metadata": {
        "id": "fpYF81fdB-Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_ang= [prompt_1_ang, prompt_2_ang, prompt_3_ang, prompt_4_ang]"
      ],
      "metadata": {
        "id": "0POG8KJ25ZuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_output_data(text, prompt):\n",
        "    if prompt in text:\n",
        "        text = text.split(prompt, 1)[1].strip()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Replace newlines with spaces\n",
        "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # Extract first sentence ending with '.', '!', or '?'\n",
        "    match = re.search(r'[^.!?]*[.!?]', text)\n",
        "    return match.group(0).strip() if match else text.strip()\n"
      ],
      "metadata": {
        "id": "T272vT4047OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Store polarity scores, labels, and confidence for each (neuron, mode) pair\n",
        "pol = defaultdict(list)\n",
        "results = []\n",
        "\n",
        "for n in anger:\n",
        "    max_act = float(max_act_df.loc[max_act_df.index == n, 'max_activation'].iloc[0])\n",
        "    print(f'Neuron: {n}, Max Activation: {max_act}')\n",
        "\n",
        "    for _ in range(20):\n",
        "            # Generate outputs\n",
        "            baseline = extract_output_data(gemma.generate(prompt_1_ang, max_new_tokens=10), prompt_1_ang)\n",
        "            steered = extract_output_data(generate_with_steering(gemma, gemma_sae, prompt_1_ang, n, max_act, -3, max_new_tokens=10), prompt_1_ang)\n",
        "            ablated = extract_output_data(generate_with_sae_ablation(gemma, gemma_sae, prompt_1_ang, n, max_new_tokens=10), prompt_1_ang)\n",
        "            steered_all = extract_output_data(generate_with_steering(gemma, gemma_sae, prompt_1_ang, anger, max_act, -3, max_new_tokens=10), prompt_1_ang)\n",
        "            ablated_all = extract_output_data(generate_with_sae_ablation(gemma, gemma_sae, prompt_1_ang, anger, max_new_tokens=10), prompt_1_ang)\n",
        "\n",
        "            # Iterate through all modes and outputs\n",
        "            for mode, output in zip(['baseline', 'steered', 'ablated', 'steered_all', 'ablated_all'],\n",
        "                                    [baseline, steered, ablated, steered_all, ablated_all]):\n",
        "\n",
        "                # Polarity scores\n",
        "                #polarity = sia.polarity_scores(output)\n",
        "                # HuggingFace-style sentiment prediction\n",
        "                #sent = sentiment_pipeline(output)[0]\n",
        "                label, score = sent(output)\n",
        "                # Store in pol\n",
        "                '''\n",
        "                pol[(n, mode)].append({\n",
        "                    'polarity': polarity,\n",
        "                    'label': sent['label'],\n",
        "                    'score': sent['score']\n",
        "                })\n",
        "                '''\n",
        "                # Store full result row\n",
        "                results.append({\n",
        "                    'neuron': n,\n",
        "                    'mode': mode,\n",
        "                    'prompt': prompt_1_ang,\n",
        "                    'output': output,\n",
        "                    #'sentiment_label': sent['label'],\n",
        "                    #'sentiment_score': sent['score'],\n",
        "                    #'compound': polarity['compound'],\n",
        "                    #'neg': polarity['neg'],\n",
        "                    #'neu': polarity['neu'],\n",
        "                    #'pos': polarity['pos'],\n",
        "                    'label': label,\n",
        "                    'score' : score,\n",
        "                })\n",
        "\n"
      ],
      "metadata": {
        "id": "qKzlmh0E4mAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_html_tags(text):\n",
        "    # Remove all HTML-like tags\n",
        "    clean_text = re.sub(r'<[^>]+>', '', text)\n",
        "    return clean_text\n"
      ],
      "metadata": {
        "id": "C3_-_xznuLCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results)\n",
        "df"
      ],
      "metadata": {
        "id": "b5lM1ajjt1Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "def plot_neuronwise_heatmap_grid(df):\n",
        "    df['label'] = df['label'].str.upper()\n",
        "    neurons = df['neuron'].unique()\n",
        "    modes = df['mode'].unique()\n",
        "    labels = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']\n",
        "\n",
        "\n",
        "    total_counts = df.groupby(['neuron', 'mode']).size().reset_index(name='total')\n",
        "\n",
        "\n",
        "    label_counts = df.groupby(['neuron', 'mode', 'label']).size().reset_index(name='count')\n",
        "\n",
        "    full_grid = pd.MultiIndex.from_product([neurons, modes, labels], names=['neuron', 'mode', 'label']).to_frame(index=False)\n",
        "\n",
        "\n",
        "    merged = pd.merge(full_grid, label_counts, on=['neuron', 'mode', 'label'], how='left').fillna({'count': 0})\n",
        "\n",
        "\n",
        "    merged = pd.merge(merged, total_counts, on=['neuron', 'mode'], how='left')\n",
        "    merged['total'] = merged['total'].fillna(1)\n",
        "\n",
        "\n",
        "    merged['percent'] = 100 * merged['count'] / merged['total']\n",
        "\n",
        "\n",
        "    n_neurons = len(neurons)\n",
        "    cols = 2\n",
        "    rows = math.ceil(n_neurons / cols)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(10, rows * 3))  # smaller plots\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, neuron in enumerate(neurons):\n",
        "        subset = merged[merged['neuron'] == neuron]\n",
        "        pivot = subset.pivot(index='label', columns='mode', values='percent').fillna(0)\n",
        "\n",
        "        sns.heatmap(pivot, ax=axes[idx], annot=True, fmt=\".1f\", cmap=\"RdBu_r\", cbar=False)\n",
        "        axes[idx].set_title(f'Neuron {neuron}')\n",
        "        axes[idx].set_xlabel('Mode')\n",
        "        axes[idx].set_ylabel('Label')\n",
        "\n",
        "\n",
        "    for j in range(idx + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_neuronwise_heatmap_grid(df)"
      ],
      "metadata": {
        "id": "UmlypzrrzzW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_compound_scores(df, text_column='output'):\n",
        "    df['compound'] = df[text_column].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "    return df\n",
        "\n",
        "df = add_compound_scores(df, text_column='output')\n"
      ],
      "metadata": {
        "id": "_uDrxCCd5MnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def plot_score_heatmap(df):\n",
        "\n",
        "    df['label'] = df['label'].str.upper()\n",
        "\n",
        "\n",
        "    pivot = df.groupby(['neuron', 'mode'])['compound'].mean().reset_index()\n",
        "    pivot_table = pivot.pivot(index='neuron', columns='mode', values='compound').fillna(0)\n",
        "\n",
        "    pivot_table = pivot_table.sort_index()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Avg NEGATIVE Score'})\n",
        "    plt.title('Average Compound Score')\n",
        "    plt.xlabel('Mode')\n",
        "    plt.ylabel('Neuron')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_score_heatmap(df)"
      ],
      "metadata": {
        "id": "Cp02AGmA3lpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('neg_steer_abl_anger_min_2.csv', index=False)"
      ],
      "metadata": {
        "id": "Z2-mfbF84Sdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}